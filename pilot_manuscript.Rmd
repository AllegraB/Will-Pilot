---
title             : "Moral Foundations of U.S. Political News Organizations"
shorttitle        : "MORAL FOUNDATIONS OF U.S. NEWS"

author: 
  - name          : "William E. Padfield"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "901 S. National Ave, Springfield, MO, 65897"
    email         : "Padfield94@live.missouristate.edu"
  - name          : "Erin M. Buchanan, PhD."
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Missouri State University"
  
author_note: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  Enter abstract here. Each new line herein must be indented, like this line.
  
keywords          : "keywords"
wordcount         : "X"

#bibliography      : ["r-references.bib"] #turn this off right now so it knits

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---

```{r load_packages, include = FALSE}
library("papaja")
```

# Method

## Sources

Political articles were scraped from the websites of four notable U.S. news sources. The sources were *The New York Times*, *National Public Radio (NPR)*, *Fox News*, and *Breitbart*. They were selected for their widespread recognition and the fact they are easily categorized (by the general public) according to perceived political lean. In general, *The New York Times* and *NPR* are perceived by many to have a liberal bias or lean. In contrast, Fox News and Breitbart are believed to have a conservative bias or lean. Political articles in particular were identified and subsequently scraped by including the specific URL directing to each source's political content in the *R* script. For example, rather than scrape from nytimes.com, which would return undesired results (non-political features, reviews, etc.), we instead included nytimes.com/section/politics so that more or less exclusively political content was obtained.

Identification of the sources' poltical URLs presented a problem for two of the sources owing to complications with how their particular sites were structured. While in the multi-week process of scraping articles, we noticed word counts for *NPR* and *Fox News* were not growing at a similar pace as those from *The New York Times* and *Breitbart*. Upon investigation, we found another, more robust URL for polticial content from NPR: their poltics content "archive." The page structure on NPR's website was such that only a limited selection of articles is displayed to the user at a given time. Scraping both the archive and the normal politics page ensured we were obtaining most (if not all) new articles as they were published. We later ran a process in order to exclude any duplicate articles. *Fox News* presented a similar issue. We discovered *Fox News* utilized six URLs in addition to the regular politics page. These URLs led to pages containing content pertaining the U.S. Executive Branch, Senate, House of Representatives, Judicial Branch, foreign policy, and elections. Once again, duplicates were subsequently eliminated from any analyses.  

## Material

## Procedure
Using the 'rvest' library in the statistical package *R*, we pulled body text for individual articles from each of the aforementioned sources (identified using CSS language) and compiled them into a dataset. Using this dataset, we identified word count and average word count per source. This process was run once daily starting on *DATE* until *DATE.* Starting on *DATE,* the process was run twice daily - once inthe morning and again in the evening. Data collection was terminated once 250,000 words per source was collected on *DATE.*  


```{r scraping, eval=FALSE, include=FALSE}
library(rvest)
####NY Times####
#Specifying the url for desired website to be scrapped
url <- 'https://www.nytimes.com/section/politics'

#Reading the HTML code from the website - headlines
webpage <- read_html(url)
headline_data = html_nodes(webpage,'.story-link a, .story-body a')

attr_data = html_attrs(headline_data) 
attr_data

urlslist = unlist(attr_data)
urlslist = urlslist[grep("http", urlslist)]
urlslist = unique(urlslist)
urlslist

##start a data frame
NYtimesDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(NYtimesDF) = c("Source", "Url", "Text")
NYtimesDF = as.data.frame(NYtimesDF)

##for loops
for (i in 1:length(urlslist)){
  
  ##read in the URL
  webpage <- read_html(urlslist[i])
  
  ##pull the specific nodes
  headline_data = html_nodes(webpage,'.story-content') 
  
  ##pull the text
  text_data = html_text(headline_data)
  
  ##save the data
  NYtimesDF$Source[i] = "NY Times"
  NYtimesDF$Url[i] = urlslist[i]
  NYtimesDF$Text[i] = paste(text_data, collapse = "")
    } ##end for loop

####NPR original front page####
url2 = 'https://www.npr.org/sections/politics/'
webpage2 = read_html(url2)
headline_data2 = html_nodes(webpage2,'.title a')

#URLs
attr_data2 = html_attrs(headline_data2) 
attr_data2

urlslist2 = unlist(attr_data2)
urlslist2 = urlslist2[grep("http", urlslist2)]
urlslist2

##start a data frame
NPRDF = matrix(NA, nrow = length(urlslist2), ncol = 3)
colnames(NPRDF) = c("Source", "Url", "Text")
NPRDF = as.data.frame(NPRDF)

##for loops
for (i in 1:length(urlslist2)){
  
  ##read in the URL
  webpage2 <- read_html(urlslist2[i])
  
  ##pull the specific nodes
  headline_data2 = html_nodes(webpage2,'#storytext > p') 
  
  ##pull the text
  text_data2 = html_text(headline_data2)
  
  ##save the data
  NPRDF$Source[i] = "NPR"
  NPRDF$Url[i] = urlslist2[i]
  NPRDF$Text[i] = paste(text_data2, collapse = "")
} ##end for loop

####Fox News####
url3 = 'http://www.foxnews.com/politics.html'
url3.1 = 'http://www.foxnews.com/category/politics/executive.html'
url3.2 = 'http://www.foxnews.com/category/politics/senate.htm.html'
url3.3 = 'http://www.foxnews.com/category/politics/house-representatives.html'
url3.4 = 'http://www.foxnews.com/category/politics/judiciary.html'
url3.5 = 'http://www.foxnews.com/category/politics/foreign-policy.html'
url3.6 = 'http://www.foxnews.com/category/politics/elections.html'
webpage3 = read_html(url3)
webpage3.1 = read_html(url3.1)
webpage3.2 = read_html(url3.2)
webpage3.3 = read_html(url3.3)
webpage3.4 = read_html(url3.4)
webpage3.5 = read_html(url3.5)
webpage3.6 = read_html(url3.6)

headline_data3 = html_nodes(webpage3, '.story- a , .article-list .title a')
headline_data3.1 = html_nodes(webpage3.1, '.story- a , .article-list .title a')
headline_data3.2 = html_nodes(webpage3.2, '.story- a , .article-list .title a')
headline_data3.3 = html_nodes(webpage3.3, '.story- a , .article-list .title a')
headline_data3.4 = html_nodes(webpage3.4, '.story- a , .article-list .title a')
headline_data3.5 = html_nodes(webpage3.5, '.story- a , .article-list .title a')
headline_data3.6 = html_nodes(webpage3.6, '.story- a , .article-list .title a')

#headline_data = html_text(headline_data)
head(headline_data3) 

attr_data3 = html_attrs(headline_data3) 
attr_data3.1 = html_attrs(headline_data3.1) 
attr_data3.2 = html_attrs(headline_data3.2) 
attr_data3.3 = html_attrs(headline_data3.3) 
attr_data3.4 = html_attrs(headline_data3.4) 
attr_data3.5 = html_attrs(headline_data3.5) 
attr_data3.6 = html_attrs(headline_data3.6) 

attr_data3

urlslist3 = c(unlist(attr_data3), unlist(attr_data3.1), 
              unlist(attr_data3.2), unlist(attr_data3.3), 
              unlist(attr_data3.4), unlist(attr_data3.5, attr_data3.6))
##find all that are href
urlslist3 = urlslist3[grep("http|.html", urlslist3)]
##fix the ones without the leading foxnews.com
urlslist3F = paste("http://www.foxnews.com", urlslist3[grep("^http", urlslist3, invert = T)], sep = "")
urlslist3N = urlslist3[grep("^http", urlslist3)]
urlslist3 = c(urlslist3N, urlslist3F)
urlslist3 = unique(urlslist3)
urlslist3

##start a data frame
FoxDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(FoxDF) = c("Source", "Url", "Text")
FoxDF = as.data.frame(FoxDF)

##for loops
for (i in 1:length(urlslist3)){
  
  ##read in the URL
  webpage3 <- read_html(urlslist3[i])
  
  ##pull the specific nodes
  headline_data3 = html_nodes(webpage3,'p+ p , .fn-video+ p , .twitter+ p , .speakable') 
  
  ##pull the text
  text_data3 = html_text(headline_data3)
  
  ##save the data
  FoxDF$Source[i] = "Fox News"
  FoxDF$Url[i] = urlslist3[i]
  FoxDF$Text[i] = paste(text_data3, collapse = "")
} ##end for loop

####Breitbart####
url4 = 'http://www.breitbart.com/big-government/'
webpage4 = read_html(url4)
headline_data4 = html_nodes(webpage4, '.title a , #grid-block-0 span , #BBTrendUL a , #disqus-popularUL a , font')
#headline_data4 = html_text(headline_data4)
head(headline_data4) 

attr_data4 = html_attrs(headline_data4) 
attr_data4

urlslist4 = unlist(attr_data4)
##find all that are href
urlslist4 = urlslist4[grep("http|/big-government", urlslist4)]
##fix the ones without the leading bb
urlslist4F = paste("http://www.breitbart.com", urlslist4[grep("^http", urlslist4, invert = T)], sep = "")
urlslist4N = urlslist4[grep("^http", urlslist4)]
urlslist4 = c(urlslist4N, urlslist4F)
urlslist4 = unique(urlslist4)
urlslist4

##start a data frame
BreitbartDF = matrix(NA, nrow = length(urlslist4), ncol = 3)
colnames(BreitbartDF) = c("Source", "Url", "Text")
BreitbartDF = as.data.frame(BreitbartDF)

##for loops
for (i in 1:length(urlslist4)){
  
  ##read in the URL
  webpage4 <- read_html(urlslist4[i])
  
  ##pull the specific nodes
  headline_data4 = html_nodes(webpage4,'.entry-content p , h2') 
  
  ##pull the text
  text_data4 = html_text(headline_data4)
  
  ##save the data
  BreitbartDF$Source[i] = "Breitbart"
  BreitbartDF$Url[i] = urlslist4[i]
  BreitbartDF$Text[i] = paste(text_data4, collapse = "")
} ##end for loop

####NPR Archive####
url5 = 'https://www.npr.org/sections/politics/archive'
webpage5 = read_html(url5)
headline_data5 = html_nodes(webpage5,'.title a')
#headline_data = html_text(headline_data)
#head(headline_data)

#URLs
attr_data5 = html_attrs(headline_data5) 
attr_data5

urlslist5 = unlist(attr_data5)
urlslist5 = urlslist5[grep("http", urlslist5)]
urlslist5

##start a data frame
NPRArchiveDF = matrix(NA, nrow = length(urlslist5), ncol = 3)
colnames(NPRArchiveDF) = c("Source", "Url", "Text")
NPRArchiveDF = as.data.frame(NPRArchiveDF)

##for loops
for (i in 1:length(urlslist5)){
  
  ##read in the URL
  webpage5 <- read_html(urlslist5[i])
  
  ##pull the specific nodes
  headline_data5 = html_nodes(webpage5,'#storytext > p') 
  
  ##pull the text
  text_data5 = html_text(headline_data5)
  
  ##save the data
  NPRArchiveDF$Source[i] = "NPR"
  NPRArchiveDF$Url[i] = urlslist5[i]
  NPRArchiveDF$Text[i] = paste(text_data5, collapse = "")
} ##end for loop

####put together####
##set your working directory
setwd("~/OneDrive - Missouri State University/RESEARCH/2 projects/Will-Pilot")

##import the overalldata 
overalldata = read.csv("overalldata.csv")

##combine with new data (add the other DF names in here...)
newdata = rbind(overalldata, NYtimesDF, NPRDF, FoxDF, BreitbartDF, NPRArchiveDF)

#temp NPR updates
#newdata = newdata[ , -4]
#newdata = rbind(newdata, NPRArchiveDF)

#change politics archive to NPR, so we can eliminate dupes
#newdata$Source[newdata$Source == "NPR Politics Archive"] = "NPR"
#newdata$Source = droplevels(newdata$Source)

##make the newdata unique in case of overlap across days
newdata = unique(newdata)

##write it back out
write.csv(newdata, "overalldata.csv", row.names = F)

##number of articles
table(newdata$Source)

##number of words
library(ngram)
newdata$Text = as.character(newdata$Text)
for (i in 1:nrow(newdata)) {
  
  #newdata$writing[i] = preprocess(newdata$Text[i], case="lower", remove.punct=TRUE)
  newdata$wordcount[i] = string.summary(newdata$Text[i])$words
  
}

tapply(newdata$wordcount, newdata$Source, mean)
tapply(newdata$wordcount, newdata$Source, sum)

```


## Data analysis

Once data collection ended, the text was scanned using the ngram package in *R*. This was done in order to remove articles that came through as blank text, as well as to eliminate text picked up from the Disqus commenting system used by certain websites. At this point, duplicate articles were discarded.
```{r data-cleanup, eval=FALSE, include=FALSE}
##pull in the data
master = read.csv("overalldata.csv", stringsAsFactors = F)

##get rid of the blank stuff
library(ngram)

for (i in 1:nrow(master)) {
  master$wordcount[i] = wordcount(master$Text[i])
}

nozero = subset(master, wordcount > 0)
tapply(nozero$wordcount, nozero$Source, mean)
tapply(nozero$wordcount, nozero$Source, sum)

##take out the disqus ones
nodis = nozero[ -c(grep("disqus", nozero$Url)), ]

##figure out the duplicates
nodis$URLS = duplicated(nodis$Url)

##all data
final = subset(nodis, URLS == FALSE)

tapply(final$wordcount, final$Source, mean)
tapply(final$wordcount, final$Source, sum)

#write.csv(final, "finaldata.csv")

####start here####
final = read.csv("finaldata.csv", stringsAsFactors = F)
final = na.omit(final) #one weird NA line

```
In order to fully capture the evaluative power of the Moral Foundations Dictionary (MFD; *CITE*), forms and conjugations of words were added to the original MFD. For example, *abuse* was supplemented with *abusive*, *abuser*, *abused*, and *abusing.* Once additional word forms were added, the words making up each of the five foundations in the MFD were split among positive and negative dimensions (i.e. *abide* as a postive authority/respect word and *defect* as a negative authority/respect word).
```{r fix-MFD, eval=FALSE, include=FALSE}
##figure out the percent of MFD words

##the amount of time people used the original MFD words
original_mfd = read.csv("original_mfd.csv", stringsAsFactors = F)

##adding forms/conjugations of words to original_mfd
#harm/care
original_mfd[c(27:95),1] = c("abusive","abuser","abused","abusing",
                             "sympathetic","sympathies","damaged",
                             "damaging","attacked","attacking","attacker",
                             "attacks","attackers", "benefits", "benefitted",
                             "benefitting", "cares", "cared", "caring", "crushes",
                             "crushing", "crushed", "dangerous", "defends", "defending",
                             "defended", "detroys", "destroyed", "destroying",
                             "fights", "fought", "fighting", "guards", "guarded",
                             "guarding", "harms", "harmed", "harming", "hurts",
                             "hurting", "kills", "killed", "killing", "preserves",
                             "preserved", "preserving", "protects", "protected",
                             "protecting", "protection", "ruins", "ruined", "ruining",
                             "safely", "safer", "shelters", "sheltered", "sheltering",
                             "spurns", "spurned", "spurning", "stomps", "stomped",
                             "stomping", "suffers", "suffered", "suffering",
                             "violence", "warring")
                             
#fairness/reciprocity
original_mfd[c(20:64),2] = c("balances", "balanced", "balancing","biased",
                             "biases", "discriminates", "discriminated",
                             "discriminating", "discrimination", "equals",
                             "equaled", "equaling", "equates", "equated",
                             "equating", "evens", "evened", "evening",
                             "excludes", "excluded", "excluding", "fairness",
                             "favors", "favored", "favoring", "honesty",
                             "impariality", "justice", "justifies", "justified",
                             "justifying", "prefers", "preferred", "preferring",
                             "prejudices", "prejudiced", "prejudicing", "reasons",
                             "reasoned", "reasoning", "rights", "tolerates",
                             "tolerated", "tolerating", "toleration")

#ingroup/loyalty
original_mfd[c(16:47),3] = c("collects", "collected", "collecting", "collective",
                             "communities", "deceives", "deceived", "deceiving",
                             "deception", "deceptions", "deserts", "deserted",
                             "deserting", "desertion", "families", "fellows",
                             "foreigners", "groups", "grouped", "grouping",
                             "indviduals", "indvidualize", "individualized",
                             "individualizing", "members", "nations", "sides",
                             "togetherness", "traits", "unites", "united", "uniting")

#authority/respect
original_mfd[c(31:112),4] = c("abides", "abided", "abiding", "authorities",
                              "classes", "classed", "classing", "command",
                              "commanded", "commanding", "controls", "controlled",
                              "controlling", "defects", "defected", "defecting",
                              "defers", "deferred", "deferring", "deference",
                              "defies", "defied", "defying", "defiance", "deserts",
                              "deserted", "deserting", "desertion", "duties",
                              "faiths", "fathers", "fathered", "fathering",
                              "honors", "honored", "honoring", "laws", "leads",
                              "leading", "mothers", "mothered", "mothering",
                              "obeys", "obeyed", "obeying", "opposes", "opposed",
                              "opposing", "orders", "ordered", "ordering", "permits",
                              "permitted", "permitting", "positions", "positioned",
                              "positioning", "preserves", "preserved", "preserving",
                              "preservation", "protests", "protested", "protesting",
                              "refuses", "refused", "refusing", "refusal", "respects",
                              "respected", "respecting", "respectful", "reveres",
                              "revered", "revering", "reverence", "serves", "served",
                              "serving", "traditions", "traditional", "traits")

#purity/sanctity
original_mfd[c(21:60),5] = c("abstains", "abstained", "abstaining", "abstinence",
                             "adulteries", "adulterous", "adulterer", "adulterers",
                             "churches", "cleans", "cleaned", "cleaning", "cleanse",
                             "cleanliness", "dirty", "diseases", "diseased", 
                             "disgusts", "disgusted", "disgusting", "grossness",
                             "innocence", "modesty", "preserves", "preserved",
                             "preserving", "preservation", "promiscuity",
                             "promiscuities", "purity", "rights", "ruins", "ruined",
                             "ruining", "sacredness", "sickness", "sicknesses",
                             "sins", "wholeness", "wholesome")

##separate them into h low, h high until you have ten categories 
#harm/care
original_mfd$h2hi[35:112] = NA 
original_mfd$h2hi[1:34] = c("benefit", "care", "defend", "guard", "preserve",
                      "protect", "safe", "shelter", "sympathetic",
                      "sympathies", "benefits", "benefitted",
                      "benefitting", "cares", "cared", "caring","defends", 
                      "defending","defended", "guards", "guarded",
                      "guarding","preserves",
                      "preserved", "preserving", "protects", "protected",
                      "protecting", "protection",  
                      "safely", "safer", "shelters", 
                      "sheltered", "sheltering")
original_mfd$h2lo[61:112] = NA
original_mfd$h2lo[1:60] = c("abuse", "attack", "cruel", "crush",
                      "damage", "danger", "destroy", "fight", "harm",
                      "hurt", "kill", "ruin", "spurn", "stomp", "suffer",
                      "violent", "war","abusive",
                      "abuser","abused","abusing","damaged",
                      "damaging","attacked","attacking","attacker",
                      "attacks","attackers","crushes",
                      "crushing", "crushed", "dangerous","detroys", 
                      "destroyed", "destroying",
                      "fights", "fought", "fighting","harms", 
                      "harmed", "harming", "hurts",
                      "hurting", "kills", "killed", "killing",
                      "ruins", "ruined", "ruining", "spurns", 
                      "spurned", "spurning", "stomps", "stomped",
                      "stomping", "suffers", "suffered", "suffering",
                      "violence", "warring")

#fairness/reciprocity
original_mfd$f2hi[41:112] = NA
original_mfd$f2hi[1:40] = c("balance", "constant", "equal", "equate", "even",
                      "fair", "honest", "just", "justify", "reason",
                      "right", "tolerate", "impartial", "balances", 
                      "balanced", "balancing",
                      "equals", "equaled", "equaling", "equates", "equated",
                      "equating", "evens", "evened", "evening","fairness",
                      "honesty", "impartiality", "justice", "justifies", 
                      "justified", "justifying", "reasons",
                      "reasoned", "reasoning", "rights", "tolerates",
                      "tolerated", "tolerating", "toleration")
original_mfd$f2lo[25:112] = NA
original_mfd$f2lo[1:24] = c("bias", "discriminate", "exclude", "favor",
                      "prefer", "prejudice", "biased", "biases", "discriminates", 
                      "discriminated", "discriminating", "discrimination",
                      "excludes", "excluded", "excluding", 
                      "favors", "favored", "favoring", "prefers", "preferred", "preferring",
                      "prejudices", "prejudiced", "prejudicing")

#ingroup/loyalty
original_mfd$i2hi[30:112] = NA
original_mfd$i2hi[1:29] = c("collect", "community", "family", "fellow",
                      "group", "member", "nation", "side", "together",
                      "trait", "unite", "collects", 
                      "collected", "collecting", "collective",
                      "communities", "families", "fellows",
                      "groups", "grouped", "grouping",
                      "members", "nations", "sides",
                      "togetherness", "traits", "unites", 
                      "united", "uniting")
original_mfd$i2lo[19:112] = NA 
original_mfd$i2lo[1:18] = c("deceive", "desert", "foreign", "individual",
                      "deceives", "deceived", "deceiving",
                      "deception", "deceptions", "deserts", "deserted",
                      "deserting", "desertion", "foreigners",
                      "indviduals", "indvidualize", "individualized",
                      "individualizing")

#authority/respect
original_mfd$a2hi[86:112] = NA
original_mfd$a2hi[1:85] = c("abide", "authority", "class", "command",
                      "control", "defer", "duty", "faith", "father", "honor",
                      "law", "lead", "legal", "mother", "obey", 
                      "order", "permit", "position", "preserve",
                      "respect", "revere", "serve", "tradtition", "trait",
                      "abides", "abided", "abiding", "authorities",
                      "classes", "classed", "classing", "command",
                      "commanded", "commanding", "controls", "controlled",
                      "controlling","defers", "deferred", "deferring", "deference",
                      "duties","faiths", "fathers", "fathered", "fathering",
                      "honors", "honored", "honoring", "laws", "leads",
                      "leading", "mothers", "mothered", "mothering",
                      "obeys", "obeyed", "obeying", "orders", 
                      "ordered", "ordering", "permits",
                      "permitted", "permitting", "positions", "positioned",
                      "positioning", "preserves", "preserved", "preserving",
                      "preservation", "respects",
                      "respected", "respecting", "respectful", "reveres",
                      "revered", "revering", "reverence", "serves", "served",
                      "serving", "traditions", "traditional", "traits")
original_mfd$a2lo[28:112] = NA
original_mfd$a2lo[1:27] = c("defect", "defy", "desert","oppose", 
                      "protest", "refuse", "defects", "defected", 
                      "defecting","defies", "defied", "defying", "defiance", "deserts",
                      "deserted", "deserting", "desertion", "opposes", "opposed",
                      "opposing", "protests", "protested", "protesting",
                      "refuses", "refused", "refusing", "refusal")

#purity/sanctity
original_mfd$p2hi[32:112] = NA
original_mfd$p2hi[1:31] = c("abstain", "church", "clean", "innocent", "modest",
                      "preserve", "pure", "right", "sacred", "whole",
                      "abstains", "abstained", "abstaining", "abstinence",
                      "churches", "cleans", "cleaned", "cleaning", "cleanse",
                      "cleanliness", "innocence", "modesty", "preserves", "preserved",
                      "preserving", "preservation", "purity", "rights", 
                      "sacredness", "wholeness", "wholesome")
original_mfd$p2lo[30:112] = NA
original_mfd$p2lo[1:29] = c("adultery", "dirt", "disease", "disgust", "gross",
                      "promiscuous", "ruin", "sick", "sin", "trash",
                      "adulteries", "adulterous", "adulterer", "adulterers",
                      "dirty", "diseases", "diseased", "disgusts", 
                      "disgusted", "disgusting", "grossness", "promiscuity",
                      "promiscuities", "ruins", "ruined","ruining", 
                      "sickness", "sicknesses", "sins")
```
Once the MFD was separated along lines of positive and negative, the article text was processed using the "tm" and "ngram" packages in *R* in order to render the text in lowercase, remove punctuation, and fix spacing issues. The individual words were then reduced to their stems (i.e. *abused* was stemmed to *abus*). The same was done to the MFD words. The article words were compiled into a dataset where they were matched up with their counterparts in the MFD along with a sum and percentage of their occurence.
```{r process-text, eval=FALSE, include=FALSE}
##load libraries
library(tm)
library(ngram)

#process the Text column (write a loop) - save processed text as a separate column
for(i in 1:nrow(final)) {
  
  final$edited[i] = preprocess(final$Text[i], #one value at a time
           case = "lower", 
           remove.punct = TRUE,
           remove.numbers = FALSE, 
           fix.spacing = TRUE)

##stem the words - do this second in the loop
  final$stemmed[i] = stemDocument(final$edited[i], language = "english")
} 

##stem the original MFD stuff - separate loop on only the mfd data frame
for(i in 1:nrow(original_mfd)) {
  original_mfd$h2hi[i] = stemDocument(original_mfd$h2hi[i], language = "english")
  original_mfd$h2lo[i] = stemDocument(original_mfd$h2lo[i], language = "english")
  original_mfd$f2hi[i] = stemDocument(original_mfd$f2hi[i], language = "english")
  original_mfd$f2lo[i] = stemDocument(original_mfd$f2lo[i], language = "english")
  original_mfd$i2hi[i] = stemDocument(original_mfd$i2hi[i], language = "english")
  original_mfd$i2lo[i] = stemDocument(original_mfd$i2lo[i], language = "english")
  original_mfd$a2hi[i] = stemDocument(original_mfd$a2hi[i], language = "english")
  original_mfd$a2lo[i] = stemDocument(original_mfd$a2lo[i], language = "english")
  original_mfd$p2hi[i] = stemDocument(original_mfd$p2hi[i], language = "english")
  original_mfd$p2lo[i] = stemDocument(original_mfd$p2lo[i], language = "english")
}

final$hhisum = NA
final$hlosum = NA
final$fhisum = NA
final$flosum = NA
final$ihisum = NA
final$ilosum = NA
final$ahisum = NA
final$alosum = NA
final$phisum = NA
final$plosum = NA

for (i in 1:nrow(final)) {
##first make a table of a response, unlisting everything
temp = as.data.frame(table(unlist(strsplit(final$stemmed[i], " "))))
##find the rows that match the mfd list and sum and save
final$hsumhi[i] = sum(temp$Freq[temp$Var1 %in% unique(original_mfd$h2hi[original_mfd$h2hi != "" & original_mfd$h2hi != "NA"])])
final$hsumlo[i] = sum(temp$Freq[temp$Var1 %in% unique(original_mfd$h2lo[original_mfd$h2lo != "" & original_mfd$h2lo != "NA"])])

final$fsumhi[i] = sum(temp$Freq[temp$Var1 %in% unique(original_mfd$f2hi[original_mfd$f2hi != "" & original_mfd$f2hi != "NA"])])
final$fsumlo[i] = sum(temp$Freq[temp$Var1 %in% unique(original_mfd$f2lo[original_mfd$f2lo != "" & original_mfd$f2lo != "NA"])])

final$isumhi[i] = sum(temp$Freq[temp$Var1 %in% unique(original_mfd$i2hi[original_mfd$i2hi != "" & original_mfd$i2hi != "NA"])])
final$isumlo[i] = sum(temp$Freq[temp$Var1 %in% unique(original_mfd$i2lo[original_mfd$i2lo != "" & original_mfd$i2lo != "NA"])])

final$asumhi[i] = sum(temp$Freq[temp$Var1 %in% unique(original_mfd$a2hi[original_mfd$a2hi != "" & original_mfd$a2hi != "NA"])])
final$asumlo[i] = sum(temp$Freq[temp$Var1 %in% unique(original_mfd$a2lo[original_mfd$a2lo != "" & original_mfd$a2lo != "NA"])])

final$psumhi[i] = sum(temp$Freq[temp$Var1 %in% unique(original_mfd$p2hi[original_mfd$p2hi != "" & original_mfd$p2hi != "NA"])])
final$psumlo[i] = sum(temp$Freq[temp$Var1 %in% unique(original_mfd$p2lo[original_mfd$p2lo != "" & original_mfd$p2lo != "NA"])])
}

final$hperhi = final$hsumhi / final$wordcount * 100
final$hperlo = final$hsumlo / final$wordcount * 100
final$fperhi = final$fsumhi / final$wordcount * 100
final$fperlo = final$fsumlo / final$wordcount * 100
final$iperhi = final$isumhi / final$wordcount * 100
final$iperlo = final$isumlo / final$wordcount * 100
final$aperhi = final$asumhi / final$wordcount * 100
final$aperlo = final$asumlo / final$wordcount * 100
final$pperhi = final$psumhi / final$wordcount * 100
final$pperlo = final$psumlo / final$wordcount * 100

#let's look at the means
tapply(final$hperhi, final$Source, mean)
tapply(final$hperlo, final$Source, mean)
tapply(final$fperhi, final$Source, mean)
tapply(final$fperlo, final$Source, mean)
tapply(final$iperhi, final$Source, mean)
tapply(final$iperlo, final$Source, mean)
tapply(final$aperhi, final$Source, mean)
tapply(final$aperlo, final$Source, mean)
tapply(final$pperhi, final$Source, mean)
tapply(final$pperlo, final$Source, mean)
```
Usung the "nlme" library in *R*, a 2 X 5 multillevel model with news source as a random intercept was run. *IMMA NEED SOME HELP WITH HOW TO STRUCTURE THIS PART :)*
```{r mlm, eval=FALSE, include=FALSE}
##Attempt at MLM
mlm_data = final[ , -c(3:28) ]
library(reshape)
long_mlm = melt(mlm_data,
                id = c("X", "Source"))
colnames(long_mlm) = c("partno", "Source", "moraltype", "percent")
long_mlm$lean = factor(long_mlm$Source,
                       levels = names(table(long_mlm$Source)),
                       labels = c("Conservative1", "Conservative",
                                  "Liberal1", "Liberal"))
long_mlm$lean = gsub("1", "", long_mlm$lean)

##set up the analysis
library(nlme)
#####intercept only model####
##gls = generalized least squares
##ML = maximum likelihood
model1 = gls(percent ~ 1, #DV ~ IV (which is only the intercept; is y-average diff than 0?)
             data = long_mlm, 
             method = "ML", 
             na.action = "na.omit")
summary(model1)

####random intercept only model####
##note we switched to LME function
model2 = lme(percent ~ 1, 
             data = long_mlm, 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|partno) #sets random intercept for each participant
summary(model2) #note changed Value in output; also, 
#Random effects tells how much intercept varies among Pps
anova(model1, model2) #tests if necessary to nest; Yes, we need to do MLM

####predictor model####
model3 = lme(percent ~ lean, #now we want to switch the 1 for IV of interest
             data = long_mlm, 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|partno)
summary(model3)
anova(model1, model2, model3)

model3.1 = lme(percent ~ lean + moraltype, #now we want to switch the 1 for IV of interest
             data = long_mlm, 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|partno)
summary(model3.1)

model3.2 = lme(percent ~ lean * moraltype, #now we want to switch the 1 for IV of interest
               data = long_mlm, 
               method = "ML", 
               na.action = "na.omit",
               random = ~1|partno)
summary(model3.2)

##model 3 on each moral foundation separately
#h_hi
model3h_hi = lme(percent ~ lean, #now we want to switch the 1 for IV of interest
             data = long_mlm[long_mlm$moraltype == "hperhi", ], 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|partno)
summary(model3h_hi)
tapply(long_mlm[long_mlm$moraltype == "hperhi", ]$percent,
       long_mlm[long_mlm$moraltype == "hperhi", ]$lean, mean)
#h_lo
model3h_lo = lme(percent ~ lean, #now we want to switch the 1 for IV of interest
                 data = long_mlm[long_mlm$moraltype == "hperlo", ], 
                 method = "ML", 
                 na.action = "na.omit",
                 random = ~1|partno)
summary(model3h_lo)
tapply(long_mlm[long_mlm$moraltype == "hperlo", ]$percent,
       long_mlm[long_mlm$moraltype == "hperlo", ]$lean, mean)
#f_hi
model3f_hi = lme(percent ~ lean, #now we want to switch the 1 for IV of interest
                 data = long_mlm[long_mlm$moraltype == "fperhi", ], 
                 method = "ML", 
                 na.action = "na.omit",
                 random = ~1|partno)
summary(model3f_hi)
tapply(long_mlm[long_mlm$moraltype == "fperhi", ]$percent,
       long_mlm[long_mlm$moraltype == "fperhi", ]$lean, mean)
#f_lo
model3f_lo = lme(percent ~ lean, #now we want to switch the 1 for IV of interest
                 data = long_mlm[long_mlm$moraltype == "fperlo", ], 
                 method = "ML", 
                 na.action = "na.omit",
                 random = ~1|partno)
summary(model3f_lo)
tapply(long_mlm[long_mlm$moraltype == "fperlo", ]$percent,
       long_mlm[long_mlm$moraltype == "fperlo", ]$lean, mean)
#i_hi
model3i_hi = lme(percent ~ lean, #now we want to switch the 1 for IV of interest
                 data = long_mlm[long_mlm$moraltype == "iperhi", ], 
                 method = "ML", 
                 na.action = "na.omit",
                 random = ~1|partno)
summary(model3i_hi)
tapply(long_mlm[long_mlm$moraltype == "iperhi", ]$percent,
       long_mlm[long_mlm$moraltype == "iperhi", ]$lean, mean)
#i_lo
model3i_lo = lme(percent ~ lean, #now we want to switch the 1 for IV of interest
                 data = long_mlm[long_mlm$moraltype == "iperlo", ], 
                 method = "ML", 
                 na.action = "na.omit",
                 random = ~1|partno)
summary(model3i_lo)
tapply(long_mlm[long_mlm$moraltype == "iperlo", ]$percent,
       long_mlm[long_mlm$moraltype == "iperlo", ]$lean, mean)
#a_hi
model3a_hi = lme(percent ~ lean, #now we want to switch the 1 for IV of interest
                 data = long_mlm[long_mlm$moraltype == "aperhi", ], 
                 method = "ML", 
                 na.action = "na.omit",
                 random = ~1|partno)
summary(model3a_hi)
tapply(long_mlm[long_mlm$moraltype == "aperhi", ]$percent,
       long_mlm[long_mlm$moraltype == "aperhi", ]$lean, mean)
#a_lo
model3a_lo = lme(percent ~ lean, #now we want to switch the 1 for IV of interest
                 data = long_mlm[long_mlm$moraltype == "aperlo", ], 
                 method = "ML", 
                 na.action = "na.omit",
                 random = ~1|partno)
summary(model3a_lo)
tapply(long_mlm[long_mlm$moraltype == "aperlo", ]$percent,
       long_mlm[long_mlm$moraltype == "aperlo", ]$lean, mean)
#p_hi
model3p_hi = lme(percent ~ lean, #now we want to switch the 1 for IV of interest
                 data = long_mlm[long_mlm$moraltype == "pperhi", ], 
                 method = "ML", 
                 na.action = "na.omit",
                 random = ~1|partno)
summary(model3p_hi)
tapply(long_mlm[long_mlm$moraltype == "pperhi", ]$percent,
       long_mlm[long_mlm$moraltype == "pperhi", ]$lean, mean)
#p_lo
model3p_lo = lme(percent ~ lean, #now we want to switch the 1 for IV of interest
                 data = long_mlm[long_mlm$moraltype == "pperlo", ], 
                 method = "ML", 
                 na.action = "na.omit",
                 random = ~1|partno)
summary(model3p_lo)
tapply(long_mlm[long_mlm$moraltype == "pperlo", ]$percent,
       long_mlm[long_mlm$moraltype == "pperlo", ]$lean, mean)
```

# Results

# Discussion


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
